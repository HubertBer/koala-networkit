\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsthm}

\title{Minimum Spanning Tree algorithms}
\author{Hubert Bernacki}
\date{April 2025}

\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=c++,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
  escapeinside={!@}{@!},
}


\begin{document}

\maketitle

\part{Introduction}

\part{Algorithms}
\section{Chazelle 1997}
\section{Chazelle 2000}
\subsection{Idea and pseudocode}
There are several ideas that work together in this algorithm and make it run in $O(m \alpha(m, n))$ time. It is important to have a certain mindset when thinking about the MST problem - thinking about it in terms of edge contractions. When you think about it this way there is an imortant duality: \\
1. The edge with smallest cost, out of all edges adjacent to a vertex, can be contracted.\\
2. If there exist a cycle in which the edge has the largest weight, then it cannot be contracted and can be discarded. \\
Most textbook algorithms (Boruvka, Prim, Kruskal) only use the first rule and approach the problem greedily. The following algorithm will be using both Boruvka and Prim algorithms, so in following sections I'll review them in the Chazelle context.
\subsubsection{Boruvka}
Let's first consider Boruvka's algorithm, it can be defined like this: \\
While the graph has edges perform boruvka-step\\

\begin{lstlisting}
boruvka(G: Graph) {
    while |E| > 0 {
        boruvka-step(G)
    }
}
\end{lstlisting}
\begin{lstlisting}
boruvka-step(G: Graph) {
    marked: set = {}
    for v: vertex in V(G) {
        marked.insert( !@ $smallest\ edge\ adjacent\ to\ v$@! )
    }
    for e: edge in marked {
        contract(e)
    }
}
\end{lstlisting}

Let's consider what happens to a connected Graph G during the boruvka-step. Let n, m be number of vertices and edges in G. We can see that the number of vertices in the resultant graph will be at most $\frac{n}{2}$, i.e. it drops by a factor of 2 [TODO EXPLAIN]. So the Boruvka algorithm calls boruvka-step only $O(log(n))$ times. Important fact is that boruvka-step can be performed in $O(n + m)$ time. Unfortunatelly the edge count m doesn't drop by a multiplicative factor, in the worst case only $\frac{n}{2}$ edges will be contracted thus Boruvka algorithm complexity is $O(m\  log(n))$. But consider what would happen if the number of edges dropped with some kind of multiplicative factor $q$, then the whole algorithm would take $\leq (n + m) + (\frac{n}{2} + qm) + (\frac{n}{4} + q^2m) ... \leq 2n + \frac{1}{1-q} m = O(n + m)$ time. So if we found a way to remove some factor of edges in linear or almost linear time, then we could also perform 1 boruvka step in linear time to also drop the number of vertices by some factor and have a routine that works in linear or slightly larger time.

Now let's consider Prim's algorithm it can be defined like this:
\subsubsection{Prim}
\begin{lstlisting}
prim(G){
    v: vertex = random-vertex(G)
    while |E(G)| > 0 {
        e: edge = smallest-edge-adjacent-to(v)
        contract-edge(e) // let's say that it contracts other vertex u to the v vertex, and v exist after the contraction
    }
}
\end{lstlisting}
We can think about it as picking a vertex v and sequentially contracting entire graph to it edge by edge. It also can be thought of as a Dijkstra analogue, we have visited and unvisited nodes and we always visit the closest one (but not the closest one to the origin v like in Dijkstra). In Prim we don't actually care about the already visited vertices and, as they were processed during their incoming edge contraction.

\subsubsection{T hierarchy}
Looking back at Boruvka we can consider the sequence of graphs which are produced during consecutive boruvka-steps. Each consecutive graph is a minor of the previous one, as boruvka-step only performs edge contractions. Minor's are transitive so resultant graph after any number of boruvka-steps will be a minor of every previous one. \\ 
Now looking at just one boruvka-step we can also think about it as of identifying multiple vertices as one, in this spirit we could create a forest which has two sets of vertices - first set corresponds to vertices in original graph and the second one has vertices from the resultant minor [TODO IMAGE]. Now we can connect vertices v from the first set and w from the second if w was created by contracting v and some other vertices. Now if we extended that forest by adding sets o vertices corresponding to each minor in boruvka step we would get a tree T, which root represents a contraction of every vertex - the final fully processed graph. Each layer below it coresponds to the previous minor, and leafs of this tree T are just the vertices of the original graph G for which we want to find the mst.

Now the hierarchy T constructed by the Boruvka algorithm has an important property, if you look at any node in hierarchy it represents a certain set of vertices (the ones identified by the leaves below this node). These vertices comprise so called contractible subgraph of G. The notion of the contractible subgraphs will be important in further analysis of the algorithm, so let's briefly discuss them.

\subsubsection{contractible subgraph}
We say that a connected induced subgraph of graph G is contractible iff it's mst is a subset of mst of G or in other words their intersection is a tree.
[TODO IMAGE]


Now if we had a partition of the vertices of a graph G into sets {$V_1, V_2, ..., V_k$} and corresponding induced subgraphs {$G_1, G_2, ..., G_k$} and let's say that G' is a graph G after contracting each of the $V_i$ sets of vertices. Now we can see that thanks to the definition of contractible subgraphs we have that \\ MST(G) = $\cup\limits_{i=1}^{k}$ (MST($V_i$)) $\cup$ MST(G') \\ Having that we can recursivly apply our mst algorithm and just sum the results.

Now looking back at the T hierarchy, if we have 

Looking back at Boruvka algorithm we can see that it produces a balanced T hierarchy with height maximally $log(n)$. But unfortunatelly this doesn't help us because computing T hierarchy using Boruvka solves mst in the process.


\subsubsection{Strongly contractible subgraph}
A subset of vertices $U \subset V(G)$ which defines an induced connected subgraph $H$, we say that $H$ is strongly contractible subgraph of $G$ iff for every pair of edges $e,f \in E(G)$, which have only 1 vertex in the subgraph $U$, we have that there exists a path from $e$ to $f$ using only edges from $H$ along which no edge has greater cost then both $e$ and $f$. 

If a subgraph $H$of $G$ is strongly contractible then it is contractible. Proof:
Let's assume $H$ is not contractible, thus $mst(G)\ \cap\ H$ has to have multiple connected components. Let's take a shortest path in mst($G$) that connects these components. It's the shortest so it cannot have edges in $H$, let $e, f$ be the first and last edge i.e. the edges which connect $H$ to the $G/H$. But the subgraph $H$ is connected and strongly contractible, so there is a path that completes the cycle and lies fully inside $H$ and has all of the edge costs smaller than the larger of the $e, f$. So the largest edge on this cycle lies outside the path in $H$, and it cannot be in the mst. Contradiction. 

\subsubsection{Reverse the construction schedule}
One of the main ideas behind the algorithm is to reverse the MST construction schedule, first we create a hierarchy T in such a way that each layer represents a partition of the graph G into contractible subgraphs. Then we can calculate the mst recursivly calling the mst on each node of T and summing the results into one mst. To explain it more thoroughly let's consider one vertex [TODO]

\subsubsection{Soft Heap}
Soft heap is a minimum heap devised by Chazelle, which is a workhorse for the algorithm. It breaks the traditional $O(log(n))$ bound for min-heaps, but at a cost of corruption - some elements may have their keys increased. Each element inserted into the soft heaps has it's original key, and also a ckey - they are equal in the beginning. The Soft heap maintains an ordering using the ckeys and ckeys cannot be lower then keys. Soft heap allows for make-heap, insert, meld, pop-min and delete operations. Each of them runs in O(1) amortized time, but for the insert operation which runs in $O(log (\frac{1}{\epsilon}))$ amortized time. The $\epsilon$ parameter is set in the make-heap and the Soft Heap maintains an invariant that there are at most $\epsilon n$ corrupted elements in the Soft Heap - where n is the number of inserted elements (not the current number of elements).  

\subsubsection{Pseudocodes}
Here are the pseudocodes for the algorithm and it's main functions:
\begin{lstlisting}
Graph mst(G: Graph, t: int) {
    if t == 1 || n == O(1) { return boruvka(G) } // STEP 1
    G0, F0 := boruvka-steps(G, c) // STEP 2
    T, B := construct-T-hierarchy(G0) // STEP 3
    F := !@$ \cup_{C_z \in T} $@! mst(!@$C_z / B$@!, t - 1) // STEP 4
    return F0 + mst(F + B, t) // STEP 5
}
\end{lstlisting}
Here is the basic pseudocode for the whole algorithm, it takes a Graph G and value t as an input and returns the MST of the graph G. Value t is used in creation of T hierarchy [TODO]. \\
STEP 1: Just runs the Boruvka algorithm if the graph is small enough or $t = 1$. \\
STEP 2 Applies c consecutive boruvka-step 's to graph G and returns the resultant \\minor - G0 and the forest of contracted edges F0.  
STEP 3 This step is the most difficult - here lies the most of the actual algorithm. Pseudocode and it's analysis below. \\
STEP 4 Recursively calls mst on T hierarchy vertices, which represent minors of subgraphs of G0.  Accumulates the resultant MSTs to one MSF F. \\
STEP 5 Recursively calls mst on a graph created from bad edges B and forest F. \\

\begin{lstlisting}
(Graph, Graph) construct-T-hierarchy(G0: Graph) {
    pick-starting-vertex()
    while edges-on_heaps {
        if met-desired-size() {
            retraction();
        } else {
            edge: Edge = heaps.pop()
            if need-fusion() { fusion(edge) }
            extension(edge)
        }
    }
}
\end{lstlisting}
First let's consider what is the goal of this function and how we want to acomplish it.

We want to create a hierarchy T - a tree which leaves represent vertices in G0, and each internal node $C_z$ represents a contraction of all leaves below it and also looking at its children it represents a graph graph minor of subgraph induced on the leaves of G0 below it where the vertices are identified with children of $C_z$ and edges map to some other original edge from $G0$ .

We will create it in postorder drvien by a stack which contains an active path that is under construction. We will denote $z_1,...z_k$ - the active path in which $z_1$ is the root of $T$. Now the T hierarchy we're creating has some bounds on number of leaves and children of each $z_i$ - that's why there is the retraction operation. When the $z_k$ attains it's desired size we retract it (that's the stack pop) by contracting the $C_{z_i}$ to one vertex and we put this vertex in $C_{z_{i-1}}$. After the retraction the active path is now $z_1, ... z_{k-1}$ and $z_{k-1}$ has now one more vertex inside. \\
Now if the $z_k$ still want's to have more vertices inside we would ideally want to do an extension - find the smallest border edge $e = (u, v)$ originating from leaves of $C_{z_k}$ (vertex $u$)  and push new $z_{k+1}$ onto the active path stack, create  $C_{z_{k
+1}}$ - which has now only 1 vertex - $v$. 
% Not true - also need to write about the invariant that the edge has to be smaller than previous ones...
% That would be the extension method if the smallest edge from all of the border edges had it's origin in $C_{z_k}$, but it might not have, in which case we need to perform a fusion

That's a general overview of what happens during the construction of T hierarchy, it's similar to Prim's algorithm - while there are edges left to process (i.e. we've not processed the whole graph) we are processing  them. If the lowest node met it's desired size we perform retraction and otherwise we retrieve ht
% \begin{lstlisting}
% void retraction() {  
% }
% \end{lstlisting}

\subsection{Proof of correctness}
During STEP 3 we maintain following invariants:
\subsubsection{Inv. 1.} 
We keep an edge called a chain-link joining $C_{z_i}$ and $C_{z_{i+1}}$ for all $i < k$. It's current cost is:
i) at most that of any border edge incident to $C_{z_1} \cup \dots \cup C_{z_i}$
ii) less than the working cost of any edge joining 2 distinct $C_{z_j}$ with $j \leq i$
\subsubsection{Inv. 2.}
For all j, the border edges (u, v) with $u \in C_{z_j}$, the edge is stored in a soft heap - denoted either H(j) or in one H(i, j), where $0 \leq i < j$. No edge appears in more than one soft heap.
Membership in H(j) implies that v is incident to at least one edge stored in some $H(i, j)$.
Membership in H(i, j) implies that v is also adjacent to $C_{z_i}$ but not to any $C_{z_l}$ in betweeen (i < l < j).
It's extended to H(0, j) and it means that v is not incident to any $C_{z_l}$ with $(l < j)$.
\subsubsection{Correctness proof} 
The proof is an induction on t and n. 
For the base case - small n or t=1 the algorithm runs just the STEP 1 i.e. runs the Boruvka algorithm on the given graph which is proven to work.
Now consider the inducitve step with sufficiently large t and n (so that we don't pass the check for the STEP 1). First let G0 and F0 denote respectively a minor of G - the result of STEP 2 and the forest of contracted edges during STEP 2. By the correctness of Boruvka algorithm and boruvka steps we know that $F_0 \subseteq MSF(G)$ and $MSF(G_{0}) \cup F_{0} = MSF(G)$. As the whole function results mst(F + B, t) we need to prove that $MST(F + B) = MST(G_{0})$. To put it differently we need to prove the following lemma:
\subsubsection{Lemma 1 [Lemma 3.1. Chazelle]}
If the edge of $G_{0}$ is not bad and it lies outside of F, then it lies outside of $MST(G_0)$ 
\subsection{Running time complexity analysis}
The complexity proof will go like this:
1. We will need to bound the number of bad edges to have a bound on the recursive mst call in STEP 5 - this is done in lemma 4.1..
2. To bound the time for STEP 3 we will bound the number of soft heap inserts - that is done in lemma 4.2.
3.  Now we can use induction to prove theorem 4.3. which bounds the runtime of the entire mst algorithm.
\begin{theorem}
\end{theorem}
\subsubsection{Lemma 4.1}
The total number of bad edges produced while building T is $|B| \leq \frac{m_0}2 + d^3n_0$.
\subsubsection{Lemma 4.2}
The total number of inserts in all the heaps is at most $4m_0$.
\subsubsection{S function}
For $i, j > 0:$
$\cases{
S(1, j) = 2j \\
S(i, 1) = 2 \\
S(i, j) = S(i, j - 1) S(i-1, S(i, j-1))
}$
\subsubsection{Lemma 5.1}
If $d=c\lceil(\frac{m}n)^\frac13\rceil$ and $t = $min{$i > 0 | n \leq S(i, d)^3$}, then $t = O(\alpha(m, n)$

Proof in Chazelle paper.
\subsubsection{Theorem 4.3}
There exists a constant $b$ that for any graph $G$ and positive integer $t$ calling $mst(G, t)$ takes time at most $bt(m + d^3(n-1))$. Where $d$ is integer large enough so that $n \leq S(t, d)^3$.

\subsubsection{Theorem 4.3 proof}
The proof is by induction on $t$ and $n$

Base case:
for $n$ and $t$ small enough we only call STEP 1 i.e. compute the mst in constant time as the $n$ and $t$ are less than some constant. We need to pick b large enough so that the theorem holds.

Induction step:
Fix $n$ and $t$ and consider a graph $G$ with $n$ vertices. \\ We want to show that the following bounds hold: \\
B1. [STEP 2, 3] take at most $\frac{b}2(n+m+d^2n_0)$ time. \\
B2. STEP 4 takes at most $b(t-1)(m_0 - |B| + dn_0)$ time. \\
B3. STEP 5 takes at most $bt(n_0 - 1 + |B| + d^3(n_0 - 1)$) time. \\

Having these bounds we can finish the proof by summing them up: \\
$btm_0 + b(\frac{m}2 - m_0 + |B|) + 2btd^3n_0 + \frac{bn}2$ \\

Which we can bound further using Lemma 4.1.\\
$btm - b(m - m_0)(t - \frac12) + 3btd^3n_0 + \frac{bn}{2}$.\\

Now remember that $n_0 \leq \frac{n}{2^c}$ and realize that  $m - m_0 > n$. Using these inequalities we can finally show that it is bounded by $bt(m+ d^3(n-1))$. \\

B1.

\section{Chazelle, Rubinfeld, Trevisan 2005}
Unlike other two algorithms presented, this is a probabilistic one with randomized runtime and randomized output. The problem that this algorithm solves is only finding an estimate of the total weight of the MST, thanks to this reformulation of the original problem the algorithm can achieve sublinear performance. 

It puts additional constraint on a graph - all edges have weights from set \{1, 2, \dots $w$\}. The algorithm itself depends heavily on $\epsilon$ - relative error of the result produced and $d$ - an average degree of the given graph. Given these constants algorithm finds an approximation of the actual MST weight within a multiplicative factor of (1+$\epsilon$) in $O(dw\epsilon^2log(\frac{dw}{\epsilon}))$. 

\subsection{Idea and pseudocode}

The main idea behind the algorithm is to reduce the problem to counting connected components in subgraphs of the given graph. Denote $G^{(i)}$ a subgraph of $G$, which contains all vertices of $G$ and all edges $e$ of $G$, which weight $w(e) \leq i$. Now consider $G^{(i)}$, we know that our MST will have to have exactly $n - c(G^{(i)})$ edges with weights $> i$, where $c(G)$ = number of CCs in $G$. Let $\alpha_{i}$ - number of edges with weight $i$ in MST. We can see that The weight of MST = $M(G) = \sum\limits_{i = 1}^{w}i\alpha_{i} = \sum\limits_{l=0}^{w-1}\sum\limits_{i = l + 1}^{w}\alpha_{i}=-w + \sum\limits_{l=0}^{w-1}c(G^{(l)}) = n - w + \sum\limits_{i=1}^{w-1}c(G^{(i)})$ 

\newpage
\begin{lstlisting}
float approx_mst_weight(Graph G, float eps){
    float ans = |V(G)| - w
    for(int i = 1; i < w; ++i){
        ans += approximate_ccs(G, eps, 4/eps, i)
    }
    return ans
}
\end{lstlisting}

\subsubsection{Approximating number of connected components}
The pseudocode for the approxmiation:
\begin{lstlisting}
float approx_number_ccs(Graph G, float eps, int W, float !@$d^*$@!) {
    r := !@$O(\frac{1}{eps^2})$@!
    !@$u_1, u_2, ..,u_r$@! := uniformly choose r vertices from G.
    for each vertex !@$u_i$@! {
        !@$\beta_i := 0$@!
        (*) flip a coin
        if (
            heads 
            && #(visited vertices) !@$\leq$@! W 
            && no visited vertex !@$v$@! has !@$d_v$@! > !@$d^*$@!
        ) {
            resume BFS to double number of visited edges.
            if ( BFS terminates ) {
                if ( !@$m_{u_i} == 0$@! ) {
                    !@$\beta_i = 2$@!
                } else {
                    !@$\beta_i = 2^{\#coin\_flips}\frac{d_{u_i}}{\#edges\_visited}$@!
                }
            } else go to (*)
        }
    }
    return !@$\hat c = \frac{n}{2r}\sum\limits_{i=1}^{r}\beta_i$@!
}
\end{lstlisting}

First, assume that the average degree $d \geq 1$ (one can always add self-loops to isolated vertices to achieve it, without changing the number of connected components)
let $d_v$ denote degree of $v$, $m_v$ denote number of edges in $v$'s connected component and $c$ denote number of connected components in the graph. 
Given a graph with vertex set $V$ for every connected component $I \subseteq V$,
$\sum\limits_{u \in I}\frac{1}{2}\frac{d_u}{m_u} = 1$ and $\sum\limits_{u \in V}\frac{1}{2}\frac{d_u}{m_u} = c$, where for isolated vertices we set $\frac{d_u}{m_u} = 2$.
In the vertex degree self-loops are counted twice.

In order to estimate $c$ we will create estimator $\beta$ for $\frac{d_u}{m_u}$. We will need 2 constants - $W$ and $d^*$, where $d^*$ is an estimate for the average degree of the graph. Given these constants consider a set $S$ of vertices which are in a connected components of size $\leq W$ in which every vertex $v$ has $d_v \leq d^*$. Now for $u_i \not\in S$ we can define $\beta$ to be $0$ and for $u_i \in S$ we define $\beta_i = 2^{\lceil log(\frac{m_{u_{i}}}{d_{u_i}})\rceil} \frac{d_{u_{i}}}{m_{u_{i}}}$ with probability $2^{-\lceil log(\frac{m_{u_{i}}}{d_{u_{i}}})\rceil}$  $u_i$ (2 if $m_{u_i} = 0$). As $\beta_i \leq 2$ we can bound its variance by: \\
$var(\beta_i) \leq \mathbb{E}[\beta_i^2] \leq 2\mathbb{E}[\beta_i] = \frac{2}{n} \sum\limits_{u \in S}\frac{d_u}{m_u} \leq \frac{4c}{n}$.

Then looking at the algorithm we can bound variance of $\hat c$ like this: \\
$var (\hat c) = var (\frac{n}{2r}\sum\limits_{i=1}^{r}\beta_i) = \frac{n^2}{4r^2}\cdot r \cdot var(\beta_i) \leq \frac{nc}{r}$

Now from Chebyshev inequality we get: \\
$\mathbb{P}(|\hat c - \mathbb{E}[\hat c]| > \frac{\epsilon n}{2}) \leq \frac{4c}{\epsilon^2 r n} = O(\frac{c}{n})$, choosing $r := O(\frac{1}{\epsilon^2})$

Now when we set $W = \frac{4}{\epsilon}$, we also get
$c - \frac{\epsilon n}{2} \leq \mathbb{E} \hat c \leq c$,
because there cannot be more then $\frac{\epsilon n}{2}$ connected components with vertices not in $S$.

We now know that with constant probability, arbitrarily close to 1, our estimation for number of connected components will deviate by at most $\epsilon n$ from actual value.

To bound the running time consider the expected number of edges visited in one iteration of the for loop which is $O(d_{u_i} log(M)$ where $M$ is the maximum number of edges we can visit (at most $Wd^*$). Thus we can bound the running time by considering all $r$ iterations like this: \\
$O(r) \cdot \frac{1}{n}\sum\limits_{u \in V} d_u log(Wd^*) = O(d \cdot  r \cdot log(W d^*)) = O(d\epsilon^{-2} log(\frac{d}{\epsilon}))$

\subsubsection{ Analysis of the main algorithm}
Now based on analysis of the runtime of the algorithm for approximating the number of connected components we can analyze the runtime of the algorithm for estimating the MST weight. Looking at the algorithm we get: \\
$\mathbb{E}\hat c $



\section{Experiments}


\end{document}
